{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/spandit/miniconda3/envs/hackathon/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2FeatureExtractor, HubertModel\n",
    "\n",
    "def extract_hubert_embeddings(audio_path, layer=-1):\n",
    "    \"\"\"\n",
    "    Extract HuBERT embeddings from an audio file.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_path (str): Path to the audio file\n",
    "    layer (int): Which transformer layer to extract embeddings from (-1 for last layer)\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: HuBERT embeddings\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Load HuBERT model and feature extractor\n",
    "    model = HubertModel.from_pretrained(\"facebook/hubert-large-ll60k\")\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-large-ll60k\")\n",
    "    \n",
    "    # Resample if necessary (HuBERT expects 16kHz)\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = feature_extractor(\n",
    "        waveform.squeeze().numpy(),\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "    # Get hidden states from specified layer\n",
    "    # -1 means last layer, -2 second to last, etc.\n",
    "    hidden_states = outputs.hidden_states[layer]\n",
    "    \n",
    "    return hidden_states\n",
    "\n",
    "def get_mean_embeddings(hidden_states):\n",
    "    \"\"\"\n",
    "    Calculate mean embeddings across time dimension.\n",
    "    \n",
    "    Parameters:\n",
    "    hidden_states (torch.Tensor): HuBERT hidden states\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Mean embeddings\n",
    "    \"\"\"\n",
    "    # Average across time dimension (dim=1)\n",
    "    mean_embeddings = torch.mean(hidden_states, dim=1)\n",
    "    return mean_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLIT AUDIO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class AudioChunkInfo:\n",
    "    \"\"\"Class to hold information about an audio chunk\"\"\"\n",
    "    chunk_number: int\n",
    "    start_time: float  # in seconds\n",
    "    end_time: float    # in seconds\n",
    "    duration: float    # in seconds\n",
    "    file_path: str\n",
    "\n",
    "class MP3Splitter:\n",
    "    \"\"\"Class to handle MP3 file splitting\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 chunk_duration: int = 60, \n",
    "                 output_dir: Optional[str] = None,\n",
    "                 min_chunk_duration: int = 30):\n",
    "        \"\"\"\n",
    "        Initialize MP3Splitter\n",
    "        \n",
    "        Args:\n",
    "            chunk_duration (int): Duration of each chunk in seconds (default: 60)\n",
    "            output_dir (str): Directory to save chunks (default: None, uses input file directory)\n",
    "            min_chunk_duration (int): Minimum duration for the last chunk in seconds (default: 30)\n",
    "        \"\"\"\n",
    "        self.chunk_duration = chunk_duration * 1000  # Convert to milliseconds\n",
    "        self.min_chunk_duration = min_chunk_duration * 1000  # Convert to milliseconds\n",
    "        self.output_dir = output_dir\n",
    "    \n",
    "    def _create_output_dir(self, input_file: str) -> str:\n",
    "        \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "        if self.output_dir is None:\n",
    "            # Use input file directory and create a subdirectory with the file name\n",
    "            base_path = Path(input_file).parent\n",
    "            file_name = Path(input_file).stem\n",
    "            output_dir = base_path / f\"{file_name}_chunks\"\n",
    "        else:\n",
    "            output_dir = Path(self.output_dir)\n",
    "        \n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return str(output_dir)\n",
    "    \n",
    "    def _get_output_path(self, output_dir: str, chunk_number: int) -> str:\n",
    "        \"\"\"Generate output file path for a chunk\"\"\"\n",
    "        return str(Path(output_dir) / f\"chunk_{chunk_number:03d}.mp3\")\n",
    "    \n",
    "    def split_audio(self, input_file: str) -> List[AudioChunkInfo]:\n",
    "        \"\"\"\n",
    "        Split MP3 file into chunks\n",
    "        \n",
    "        Args:\n",
    "            input_file (str): Path to input MP3 file\n",
    "            \n",
    "        Returns:\n",
    "            List[AudioChunkInfo]: Information about each chunk\n",
    "        \"\"\"\n",
    "        # Validate input file\n",
    "        if not os.path.exists(input_file):\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = self._create_output_dir(input_file)\n",
    "        logger.info(f\"Chunks will be saved in: {output_dir}\")\n",
    "        \n",
    "        try:\n",
    "            # Load audio file\n",
    "            logger.info(\"Loading audio file...\")\n",
    "            audio = AudioSegment.from_mp3(input_file)\n",
    "            \n",
    "            # Calculate number of chunks\n",
    "            total_duration = len(audio)\n",
    "            num_chunks = math.ceil(total_duration / self.chunk_duration)\n",
    "            \n",
    "            chunk_infos = []\n",
    "            \n",
    "            # Split audio into chunks with progress bar\n",
    "            logger.info(\"Splitting audio into chunks...\")\n",
    "            for i in tqdm(range(num_chunks), desc=\"Processing chunks\"):\n",
    "                # Calculate start and end times\n",
    "                start_time = i * self.chunk_duration\n",
    "                end_time = min((i + 1) * self.chunk_duration, total_duration)\n",
    "                \n",
    "                # Skip if last chunk is too short\n",
    "                if i == num_chunks - 1 and (end_time - start_time) < self.min_chunk_duration:\n",
    "                    # Extend the previous chunk instead\n",
    "                    if chunk_infos:\n",
    "                        prev_chunk = chunk_infos[-1]\n",
    "                        os.remove(prev_chunk.file_path)  # Remove the previous chunk\n",
    "                        \n",
    "                        # Create new extended chunk\n",
    "                        extended_chunk = audio[prev_chunk.start_time * 1000:end_time]\n",
    "                        extended_chunk.export(prev_chunk.file_path, format=\"mp3\")\n",
    "                        \n",
    "                        # Update previous chunk info\n",
    "                        chunk_infos[-1] = AudioChunkInfo(\n",
    "                            chunk_number=prev_chunk.chunk_number,\n",
    "                            start_time=prev_chunk.start_time,\n",
    "                            end_time=end_time / 1000,\n",
    "                            duration=(end_time - prev_chunk.start_time * 1000) / 1000,\n",
    "                            file_path=prev_chunk.file_path\n",
    "                        )\n",
    "                    break\n",
    "                \n",
    "                # Extract chunk\n",
    "                chunk = audio[start_time:end_time]\n",
    "                \n",
    "                # Generate output path\n",
    "                output_path = self._get_output_path(output_dir, i)\n",
    "                \n",
    "                # Export chunk\n",
    "                chunk.export(output_path, format=\"mp3\")\n",
    "                \n",
    "                # Store chunk information\n",
    "                chunk_info = AudioChunkInfo(\n",
    "                    chunk_number=i,\n",
    "                    start_time=start_time / 1000,  # Convert to seconds\n",
    "                    end_time=end_time / 1000,      # Convert to seconds\n",
    "                    duration=(end_time - start_time) / 1000,  # Convert to seconds\n",
    "                    file_path=output_path\n",
    "                )\n",
    "                chunk_infos.append(chunk_info)\n",
    "            \n",
    "            logger.info(f\"Successfully created {len(chunk_infos)} chunks\")\n",
    "            return chunk_infos\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error splitting audio: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class AudioMetadata:\n",
    "    \"\"\"Class to handle audio metadata operations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_chunk_metadata(chunk_infos: List[AudioChunkInfo], output_dir: str):\n",
    "        \"\"\"Save metadata for all chunks\"\"\"\n",
    "        metadata_path = os.path.join(output_dir, \"chunks_metadata.txt\")\n",
    "        \n",
    "        with open(metadata_path, 'w') as f:\n",
    "            f.write(\"Chunk Information:\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            \n",
    "            for chunk in chunk_infos:\n",
    "                f.write(f\"Chunk {chunk.chunk_number:03d}:\\n\")\n",
    "                f.write(f\"  Start Time: {chunk.start_time:.2f} seconds\\n\")\n",
    "                f.write(f\"  End Time: {chunk.end_time:.2f} seconds\\n\")\n",
    "                f.write(f\"  Duration: {chunk.duration:.2f} seconds\\n\")\n",
    "                f.write(f\"  File: {os.path.basename(chunk.file_path)}\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Chunks will be saved in: /home/snp2453/slt/speech_retrieval/data/processed_audio\n",
      "INFO:__main__:Loading audio file...\n",
      "INFO:__main__:Splitting audio into chunks...\n",
      "Processing chunks: 100%|██████████| 225/225 [00:53<00:00,  4.22it/s]\n",
      "INFO:__main__:Successfully created 225 chunks\n",
      "INFO:__main__:Processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/home/snp2453/slt/speech_retrieval/data/raw_audio/long_news.mp3\"\n",
    "output_dir = \"/home/snp2453/slt/speech_retrieval/data/processed_audio\"\n",
    "chunk_duration = 15\n",
    "min_chunk_duration = 5\n",
    "\n",
    "try:\n",
    "    # Initialize splitter\n",
    "    splitter = MP3Splitter(\n",
    "        chunk_duration=chunk_duration,\n",
    "        output_dir=output_dir,\n",
    "        min_chunk_duration=min_chunk_duration\n",
    "    )\n",
    "    \n",
    "    # Split audio\n",
    "    chunk_infos = splitter.split_audio(input_file)\n",
    "    \n",
    "    # Save metadata\n",
    "    if chunk_infos:\n",
    "        output_dir = os.path.dirname(chunk_infos[0].file_path)\n",
    "        AudioMetadata.save_chunk_metadata(chunk_infos, output_dir)\n",
    "        \n",
    "    logger.info(\"Processing completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/home/snp2453/slt/speech_retrieval/data/processed_audio\"\n",
    "files = os.listdir(path)\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43min 13s, sys: 5min 33s, total: 48min 47s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in files:\n",
    "    full_path = os.path.join(path, i)\n",
    "    name = i.split(\".\")[0]\n",
    "    type = i.split(\".\")[1]\n",
    "    if type != \"mp3\":\n",
    "        continue\n",
    "    hidden_states = extract_hubert_embeddings(full_path)\n",
    "    mean_embeddings = get_mean_embeddings(hidden_states)\n",
    "    torch.save(mean_embeddings, f\"/home/snp2453/slt/speech_retrieval/data/embeddings/chunks_embedding/{name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 min 30 seconds vs 4 mins 44 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = extract_hubert_embeddings(\"/home/snp2453/slt/speech_retrieval/data/raw_audio/Russia_Google.mp3\")\n",
    "mean_embeddings = get_mean_embeddings(hidden_states)\n",
    "torch.save(mean_embeddings, f\"/home/snp2453/slt/speech_retrieval/data/embeddings/Russia_Google.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Union, Optional\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class SimilarityResult:\n",
    "    \"\"\"Class to hold similarity computation results\"\"\"\n",
    "    chunk_id: str\n",
    "    similarity_score: float\n",
    "    chunk_start_time: float\n",
    "    chunk_end_time: float\n",
    "    embedding_path: str\n",
    "\n",
    "class EmbeddingSimilarityCalculator:\n",
    "    \"\"\"Handles similarity computations between embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize calculator\n",
    "        \n",
    "        Args:\n",
    "            embeddings_dir (str): Directory containing stored embeddings\n",
    "        \"\"\"\n",
    "        self.embeddings_dir = Path(embeddings_dir)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def load_embedding(self, embedding_path: Union[str, Path]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Load embedding from file and process it\n",
    "        \n",
    "        Args:\n",
    "            embedding_path (Union[str, Path]): Path to embedding file\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Processed embedding tensor\n",
    "        \"\"\"\n",
    "        try:\n",
    "            embedding = torch.load(embedding_path, map_location=self.device)\n",
    "            \n",
    "            # If embedding is 3D (batch_size, sequence_length, hidden_size),\n",
    "            # take mean over sequence dimension\n",
    "            if len(embedding.shape) == 3:\n",
    "                embedding = torch.mean(embedding, dim=1)\n",
    "            \n",
    "            # Ensure embedding is 2D (batch_size, hidden_size)\n",
    "            if len(embedding.shape) == 1:\n",
    "                embedding = embedding.unsqueeze(0)\n",
    "                \n",
    "            # Normalize embedding\n",
    "            embedding = F.normalize(embedding, p=2, dim=-1)\n",
    "            \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading embedding from {embedding_path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def compute_cosine_similarity(self, \n",
    "                                query_embedding: torch.Tensor,\n",
    "                                chunk_embedding: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between query and chunk embeddings\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (torch.Tensor): Query embedding\n",
    "            chunk_embedding (torch.Tensor): Chunk embedding\n",
    "            \n",
    "        Returns:\n",
    "            float: Cosine similarity score\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            similarity = F.cosine_similarity(\n",
    "                query_embedding,\n",
    "                chunk_embedding,\n",
    "                dim=-1\n",
    "            )\n",
    "        return similarity.item()\n",
    "    \n",
    "    def process_single_chunk(self,\n",
    "                           query_embedding: torch.Tensor,\n",
    "                           chunk_embedding_path: Union[str, Path],\n",
    "                           chunk_metadata: Optional[Dict] = None) -> SimilarityResult:\n",
    "        \"\"\"\n",
    "        Process a single chunk and compute similarity\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (torch.Tensor): Query embedding\n",
    "            chunk_embedding_path (Union[str, Path]): Path to chunk embedding\n",
    "            chunk_metadata (Optional[Dict]): Chunk metadata if available\n",
    "            \n",
    "        Returns:\n",
    "            SimilarityResult: Similarity computation result\n",
    "        \"\"\"\n",
    "        # Load and process chunk embedding\n",
    "        chunk_embedding = self.load_embedding(chunk_embedding_path)\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity_score = self.compute_cosine_similarity(\n",
    "            query_embedding,\n",
    "            chunk_embedding\n",
    "        )\n",
    "        \n",
    "        # Get chunk information\n",
    "        chunk_id = Path(chunk_embedding_path).stem\n",
    "        start_time = chunk_metadata.get('start_time', 0.0) if chunk_metadata else 0.0\n",
    "        end_time = chunk_metadata.get('end_time', 0.0) if chunk_metadata else 0.0\n",
    "        \n",
    "        return SimilarityResult(\n",
    "            chunk_id=chunk_id,\n",
    "            similarity_score=similarity_score,\n",
    "            chunk_start_time=start_time,\n",
    "            chunk_end_time=end_time,\n",
    "            embedding_path=str(chunk_embedding_path)\n",
    "        )\n",
    "    \n",
    "    def find_most_similar_chunks(self,\n",
    "                               query_path: str,\n",
    "                               top_k: int = 1,\n",
    "                               metadata_path: Optional[str] = None) -> List[SimilarityResult]:\n",
    "        \"\"\"\n",
    "        Find the most similar chunks to a query\n",
    "        \n",
    "        Args:\n",
    "            query_path (str): Path to query embedding\n",
    "            top_k (int): Number of top results to return\n",
    "            metadata_path (Optional[str]): Path to chunks metadata file\n",
    "            \n",
    "        Returns:\n",
    "            List[SimilarityResult]: Top-k most similar chunks\n",
    "        \"\"\"\n",
    "        # Load query embedding\n",
    "        query_embedding = self.load_embedding(query_path)\n",
    "        \n",
    "        # Load metadata if available\n",
    "        chunk_metadata = {}\n",
    "        if metadata_path and Path(metadata_path).exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                chunk_metadata = json.load(f)\n",
    "        \n",
    "        # Process all chunk embeddings\n",
    "        results = []\n",
    "        chunk_paths = sorted(self.embeddings_dir.glob(\"chunk_*.pt\"))\n",
    "        \n",
    "        for chunk_path in tqdm(chunk_paths, desc=\"Processing chunks\"):\n",
    "            try:\n",
    "                metadata = chunk_metadata.get(chunk_path.stem, {})\n",
    "                result = self.process_single_chunk(\n",
    "                    query_embedding,\n",
    "                    chunk_path,\n",
    "                    metadata\n",
    "                )\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing chunk {chunk_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Sort by similarity score and get top-k\n",
    "        results.sort(key=lambda x: x.similarity_score, reverse=True)\n",
    "        return results[:top_k]\n",
    "\n",
    "class SimilarityVisualizer:\n",
    "    \"\"\"Handles visualization of similarity results\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_results(results: List[SimilarityResult], \n",
    "                    output_path: str):\n",
    "        \"\"\"Save similarity results to file\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"Similarity Results:\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                f.write(f\"Rank {i}:\\n\")\n",
    "                f.write(f\"  Chunk ID: {result.chunk_id}\\n\")\n",
    "                f.write(f\"  Similarity Score: {result.similarity_score:.4f}\\n\")\n",
    "                f.write(f\"  Time Range: {result.chunk_start_time:.2f}s - \"\n",
    "                       f\"{result.chunk_end_time:.2f}s\\n\")\n",
    "                f.write(f\"  Embedding: {result.embedding_path}\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_similarities(results: List[SimilarityResult],\n",
    "                        output_path: str):\n",
    "        \"\"\"Create visualization of similarity scores\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import seaborn as sns\n",
    "            \n",
    "            # Prepare data\n",
    "            chunk_ids = [r.chunk_id for r in results]\n",
    "            scores = [r.similarity_score for r in results]\n",
    "            \n",
    "            # Create plot\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x=range(len(chunk_ids)), y=scores)\n",
    "            \n",
    "            # Customize plot\n",
    "            plt.title(\"Chunk Similarity Scores\")\n",
    "            plt.xlabel(\"Chunk ID\")\n",
    "            plt.ylabel(\"Cosine Similarity\")\n",
    "            plt.xticks(range(len(chunk_ids)), chunk_ids, rotation=45)\n",
    "            \n",
    "            # Save plot\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_path)\n",
    "            plt.close()\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.warning(\"matplotlib and seaborn required for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3315212/2746021202.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embedding = torch.load(embedding_path, map_location=self.device)\n",
      "Processing chunks:   0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 225/225 [00:00<00:00, 1766.41it/s]\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:__main__:Processing completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 440 ms, sys: 133 ms, total: 573 ms\n",
      "Wall time: 342 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_dir = \"/home/snp2453/slt/speech_retrieval/results\"\n",
    "embeddings_dir = \"/home/snp2453/slt/speech_retrieval/data/embeddings/chunks_embedding\"\n",
    "query_path = \"/home/snp2453/slt/speech_retrieval/data/embeddings/Russia_Google.pt\"\n",
    "top_k = 20\n",
    "metadata_path = \"/home/snp2453/slt/speech_retrieval/chunk_metadata.json\"\n",
    "try:\n",
    "    # Create output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Initialize calculator\n",
    "    calculator = EmbeddingSimilarityCalculator(embeddings_dir)\n",
    "    # Find most similar chunks\n",
    "    results = calculator.find_most_similar_chunks(\n",
    "    query_path,\n",
    "    top_k=top_k,\n",
    "    metadata_path=metadata_path\n",
    "    )\n",
    "    # Save results\n",
    "    SimilarityVisualizer.save_results(\n",
    "    results,\n",
    "    output_dir / \"similarity_results.txt\"\n",
    "    )\n",
    "    # Create visualization\n",
    "    SimilarityVisualizer.plot_similarities(\n",
    "    results,\n",
    "    output_dir / \"similarity_plot.png\"\n",
    "    )\n",
    "    logger.info(\"Processing completed successfully!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
