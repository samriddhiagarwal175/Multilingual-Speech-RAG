{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence, detect_nonsilent\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import webrtcvad\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AudioChunker:\n",
    "    def __init__(self, sample_rate=16000, min_chunk_duration=10, max_chunk_duration=30):\n",
    "        \"\"\"\n",
    "        Initialize the audio chunker with specified sample rate and chunk duration bounds.\n",
    "        Args:\n",
    "            sample_rate (int): Sample rate of the audio (default: 16000 Hz)\n",
    "            min_chunk_duration (int): Minimum duration of any chunk in seconds (default: 10)\n",
    "            max_chunk_duration (int): Maximum duration of any chunk in seconds (default: 30)\n",
    "        \"\"\"\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_chunk_duration = min_chunk_duration\n",
    "        self.max_chunk_duration = max_chunk_duration\n",
    "        self.vad = webrtcvad.Vad(3)  # Aggressiveness mode 3 (most aggressive)\n",
    "\n",
    "    def numpy_to_audiosegment(self, audio_data):\n",
    "        \"\"\"\n",
    "        Convert numpy array to AudioSegment.\n",
    "        Args:\n",
    "            audio_data (numpy.ndarray): Audio signal (-1.0 to 1.0 float32)\n",
    "        Returns:\n",
    "            AudioSegment: Audio data as an AudioSegment object\n",
    "        \"\"\"\n",
    "        # Convert to 16-bit PCM\n",
    "        audio_int16 = (audio_data * 32768).astype(np.int16)\n",
    "        \n",
    "        # Create AudioSegment from raw PCM data\n",
    "        return AudioSegment(\n",
    "            audio_int16.tobytes(), \n",
    "            frame_rate=self.sample_rate,\n",
    "            sample_width=2,\n",
    "            channels=1\n",
    "        )\n",
    "\n",
    "    def audiosegment_to_numpy(self, audio_segment):\n",
    "        \"\"\"\n",
    "        Convert AudioSegment to numpy array.\n",
    "        Args:\n",
    "            audio_segment (AudioSegment): Audio data as AudioSegment\n",
    "        Returns:\n",
    "            numpy.ndarray: Audio signal as float32 numpy array\n",
    "        \"\"\"\n",
    "        # Get raw audio data as numpy array\n",
    "        samples = np.array(audio_segment.get_array_of_samples())\n",
    "        \n",
    "        # Convert to float32\n",
    "        return samples.astype(np.float32) / 32768.0\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"\n",
    "        Load audio file using librosa and normalize it.\n",
    "        Args:\n",
    "            file_path (str): Path to the audio file\n",
    "        Returns:\n",
    "            tuple: (audio_data, sample_rate)\n",
    "        \"\"\"\n",
    "        audio_data, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "        return audio_data, sr\n",
    "\n",
    "    def merge_small_chunks(self, chunks, min_duration):\n",
    "        \"\"\"\n",
    "        Merge chunks smaller than min_duration with adjacent chunks.\n",
    "        Args:\n",
    "            chunks (list): List of AudioSegment chunks\n",
    "            min_duration (float): Minimum duration in seconds\n",
    "        Returns:\n",
    "            list: Merged chunks as AudioSegment objects\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            return chunks\n",
    "\n",
    "        merged = []\n",
    "        current_chunk = chunks[0]\n",
    "        \n",
    "        for next_chunk in chunks[1:]:\n",
    "            current_duration = len(current_chunk.get_array_of_samples()) / self.sample_rate\n",
    "            next_duration = len(next_chunk.get_array_of_samples()) / self.sample_rate\n",
    "            \n",
    "            # If current chunk is too small, merge it with the next chunk\n",
    "            if current_duration < min_duration:\n",
    "                current_chunk = current_chunk + next_chunk\n",
    "            # If merging would exceed max duration, save current and start new\n",
    "            elif (current_duration + next_duration) > self.max_chunk_duration:\n",
    "                merged.append(current_chunk)\n",
    "                current_chunk = next_chunk\n",
    "            # If current chunk is already good size, start new chunk\n",
    "            else:\n",
    "                merged.append(current_chunk)\n",
    "                current_chunk = next_chunk\n",
    "        \n",
    "        # Don't forget the last chunk\n",
    "        if current_chunk:\n",
    "            # If the last chunk is too small and we have previous chunks, merge it with the last merged chunk\n",
    "            if (len(current_chunk.get_array_of_samples()) / self.sample_rate < min_duration) and merged:\n",
    "                merged[-1] = merged[-1] + current_chunk\n",
    "            else:\n",
    "                merged.append(current_chunk)\n",
    "        \n",
    "        return merged\n",
    "\n",
    "    def chunk_by_silence(self, audio_data, min_silence_len=500, silence_thresh=-40):\n",
    "        \"\"\"\n",
    "        Split audio into chunks based on silence detection.\n",
    "        Args:\n",
    "            audio_data (numpy.ndarray): Audio signal\n",
    "            min_silence_len (int): Minimum length of silence (in ms)\n",
    "            silence_thresh (int): Silence threshold in dB\n",
    "        Returns:\n",
    "            list: List of audio chunks as AudioSegment objects\n",
    "        \"\"\"\n",
    "        # Convert numpy array to AudioSegment\n",
    "        audio_segment = self.numpy_to_audiosegment(audio_data)\n",
    "\n",
    "        # Split on silence\n",
    "        chunks = split_on_silence(\n",
    "            audio_segment,\n",
    "            min_silence_len=min_silence_len,\n",
    "            silence_thresh=silence_thresh,\n",
    "            keep_silence=300  # Keep 300ms of silence at the start and end\n",
    "        )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def chunk_by_vad(self, audio_data, frame_duration=30):\n",
    "        \"\"\"\n",
    "        Split audio into chunks using WebRTC Voice Activity Detection.\n",
    "        Args:\n",
    "            audio_data (numpy.ndarray): Audio signal\n",
    "            frame_duration (int): Duration of each frame in milliseconds (10, 20, or 30)\n",
    "        Returns:\n",
    "            list: List of audio chunks as numpy arrays\n",
    "        \"\"\"\n",
    "        # Convert float32 audio to int16\n",
    "        audio_int16 = (audio_data * 32768).astype(np.int16)\n",
    "        \n",
    "        # Calculate frame size\n",
    "        frame_size = int(self.sample_rate * frame_duration / 1000)\n",
    "        \n",
    "        # Pad audio to ensure it's divisible by frame_size\n",
    "        if len(audio_int16) % frame_size != 0:\n",
    "            padding = frame_size - (len(audio_int16) % frame_size)\n",
    "            audio_int16 = np.pad(audio_int16, (0, padding))\n",
    "        \n",
    "        # Split audio into frames\n",
    "        frames = np.array_split(audio_int16, len(audio_int16) // frame_size)\n",
    "        \n",
    "        # Detect voice activity in each frame\n",
    "        is_speech = []\n",
    "        for frame in frames:\n",
    "            try:\n",
    "                if len(frame) == frame_size:  # Only process full-size frames\n",
    "                    is_speech.append(self.vad.is_speech(frame.tobytes(), self.sample_rate))\n",
    "                else:\n",
    "                    is_speech.append(False)\n",
    "            except:\n",
    "                is_speech.append(False)\n",
    "        \n",
    "        # Group consecutive speech frames into chunks\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        \n",
    "        for i, speech in enumerate(is_speech):\n",
    "            if speech:\n",
    "                current_chunk.extend(frames[i])\n",
    "            elif current_chunk:\n",
    "                chunks.append(np.array(current_chunk))\n",
    "                current_chunk = []\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(np.array(current_chunk))\n",
    "        \n",
    "        # Convert int16 chunks back to float32\n",
    "        return [chunk.astype(np.float32) / 32768.0 for chunk in chunks]\n",
    "\n",
    "    def adaptive_chunking(self, audio_data, min_silence_len=500, silence_thresh=-40, vad_frame_duration=30):\n",
    "        \"\"\"\n",
    "        Multi-stage chunking process:\n",
    "        1. First splits audio based on silence\n",
    "        2. If any chunk exceeds max_duration, splits it further using VAD\n",
    "        3. Merges chunks smaller than min_duration\n",
    "        \n",
    "        Args:\n",
    "            audio_data (numpy.ndarray): Audio signal\n",
    "            min_silence_len (int): Minimum length of silence (in ms)\n",
    "            silence_thresh (int): Silence threshold in dB\n",
    "            vad_frame_duration (int): Frame duration for VAD in ms\n",
    "        Returns:\n",
    "            list: List of audio chunks as AudioSegment objects\n",
    "        \"\"\"\n",
    "        # First stage: Silence-based chunking\n",
    "        silence_chunks = self.chunk_by_silence(audio_data, min_silence_len, silence_thresh)\n",
    "        \n",
    "        intermediate_chunks = []\n",
    "        max_samples = self.max_chunk_duration * self.sample_rate\n",
    "        \n",
    "        # Process chunks that are too long\n",
    "        for chunk in silence_chunks:\n",
    "            chunk_duration = len(chunk.get_array_of_samples()) / self.sample_rate\n",
    "            \n",
    "            if chunk_duration > self.max_chunk_duration:\n",
    "                # Convert AudioSegment to numpy array for VAD processing\n",
    "                chunk_np = self.audiosegment_to_numpy(chunk)\n",
    "                \n",
    "                # Second stage: VAD-based chunking for long segments\n",
    "                vad_chunks = self.chunk_by_vad(chunk_np, vad_frame_duration)\n",
    "                \n",
    "                # Further split VAD chunks if they're still too long\n",
    "                for vad_chunk in vad_chunks:\n",
    "                    if len(vad_chunk) > max_samples:\n",
    "                        # Split into fixed-size chunks if still too long\n",
    "                        num_subchunks = int(np.ceil(len(vad_chunk) / max_samples))\n",
    "                        subchunks = np.array_split(vad_chunk, num_subchunks)\n",
    "                        for subchunk in subchunks:\n",
    "                            intermediate_chunks.append(self.numpy_to_audiosegment(subchunk))\n",
    "                    else:\n",
    "                        intermediate_chunks.append(self.numpy_to_audiosegment(vad_chunk))\n",
    "            else:\n",
    "                intermediate_chunks.append(chunk)\n",
    "        \n",
    "        # Final stage: Merge chunks that are too small\n",
    "        final_chunks = self.merge_small_chunks(intermediate_chunks, self.min_chunk_duration)\n",
    "        \n",
    "        return final_chunks\n",
    "\n",
    "    def save_chunks(self, chunks, output_dir, prefix=\"chunk\"):\n",
    "        \"\"\"\n",
    "        Save audio chunks to files.\n",
    "        Args:\n",
    "            chunks (list): List of audio chunks\n",
    "            output_dir (str): Directory to save chunks\n",
    "            prefix (str): Prefix for chunk filenames\n",
    "        \"\"\"\n",
    "        import os\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            output_path = os.path.join(output_dir, f\"{prefix}_{i}.wav\")\n",
    "            chunk_duration = len(chunk.get_array_of_samples()) / self.sample_rate\n",
    "            if isinstance(chunk, AudioSegment):\n",
    "                chunk.export(output_path, format=\"wav\")\n",
    "            else:\n",
    "                sf.write(output_path, chunk, self.sample_rate)\n",
    "            print(f\"Saved chunk {i}: {chunk_duration:.2f} seconds\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize chunker with 10-second minimum and 30-second maximum chunk duration\n",
    "    chunker = AudioChunker(min_chunk_duration=10, max_chunk_duration=30)\n",
    "\n",
    "    # Load audio file\n",
    "    audio_file = \"/mnt/data/ashwin/SpeechRAG/speech_retrieval/data/raw_audio/long_news.mp3\"\n",
    "    audio_data, sr = chunker.load_audio(audio_file)\n",
    "\n",
    "    # Use adaptive chunking (silence + VAD)\n",
    "    chunks = chunker.adaptive_chunking(\n",
    "        audio_data,\n",
    "        min_silence_len=500,      # Minimum silence length for initial splitting\n",
    "        silence_thresh=-40,       # Silence threshold in dB\n",
    "        vad_frame_duration=30     # Frame duration for VAD\n",
    "    )\n",
    "\n",
    "    # Save the chunks\n",
    "    chunker.save_chunks(chunks, \"output_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
