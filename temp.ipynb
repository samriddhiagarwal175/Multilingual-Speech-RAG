{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import pandas as pd\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"json\",data_files = \"/home/snp2453/slt/LibriSQA-PartI-test.json\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def merge_audio_files_from_dataset(df, batch_size=10):\n",
    "    \"\"\"\n",
    "    Merge audio files based on dataset rows and update paths\n",
    "    \n",
    "    Args:\n",
    "        df: HuggingFace dataset\n",
    "        batch_size: Number of files to merge together (default: 10)\n",
    "    \"\"\"\n",
    "    # Convert dataset to pandas for easier manipulation\n",
    "    df_pd = df.to_pandas()\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"merged_audio_files\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(df_pd), batch_size):\n",
    "        batch = df_pd.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Skip if batch is incomplete\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "            \n",
    "        # Initialize combined audio\n",
    "        combined = None\n",
    "        \n",
    "        # Get the files in this batch\n",
    "        for _, row in batch.iterrows():\n",
    "            # Convert wav path to flac path\n",
    "            flac_path = row['speech_path'].replace('.wav', '.flac')\n",
    "            \n",
    "            try:\n",
    "                # Load the FLAC file\n",
    "                audio = AudioSegment.from_file(flac_path, format='flac')\n",
    "                \n",
    "                if combined is None:\n",
    "                    combined = audio\n",
    "                else:\n",
    "                    combined += audio\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {flac_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if combined is not None:\n",
    "            # Generate output filename based on batch index\n",
    "            batch_num = i // batch_size\n",
    "            output_wav = f\"merged_{batch_num:04d}.wav\"\n",
    "            output_path = os.path.join(output_dir, output_wav)\n",
    "            \n",
    "            # Export as WAV\n",
    "            combined.export(output_path, format='wav')\n",
    "            \n",
    "            # Update paths in the dataframe\n",
    "            new_path = os.path.join('merged_audio_files', output_wav)\n",
    "            df_pd.loc[i:i+batch_size-1, 'speech_path'] = new_path\n",
    "            \n",
    "    # Convert back to HuggingFace dataset\n",
    "    return Dataset.from_pandas(df_pd)\n",
    "\n",
    "def process_dataset(df):\n",
    "    \"\"\"\n",
    "    Main processing function\n",
    "    \n",
    "    Args:\n",
    "        df: HuggingFace dataset\n",
    "    Returns:\n",
    "        Updated dataset with new paths\n",
    "    \"\"\"\n",
    "    print(\"Starting audio file merging process...\")\n",
    "    \n",
    "    # Merge files and update dataset\n",
    "    updated_df = merge_audio_files_from_dataset(df)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = process_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_files(df, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Transcribe merged audio files using Whisper and update the dataset\n",
    "    \n",
    "    Args:\n",
    "        df: HuggingFace dataset with merged audio paths\n",
    "        use_gpu: Whether to use GPU for transcription\n",
    "    Returns:\n",
    "        Updated dataset with transcriptions\n",
    "    \"\"\"\n",
    "    # Convert dataset to pandas for easier manipulation\n",
    "    df_pd = df.to_pandas()\n",
    "    \n",
    "    # Create transcriptions directory\n",
    "    transcript_dir = \"transcriptions\"\n",
    "    if not os.path.exists(transcript_dir):\n",
    "        os.makedirs(transcript_dir)\n",
    "    \n",
    "    # Initialize Whisper\n",
    "    device = \"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    whisper = pipeline(\"automatic-speech-recognition\",\n",
    "                      \"openai/whisper-tiny\",\n",
    "                      return_timestamps=True,\n",
    "                      device=device)\n",
    "    \n",
    "    # Add new column for Whisper transcriptions\n",
    "    df_pd['whisper_transcription'] = ''\n",
    "    \n",
    "    # Get unique merged audio files from the dataset\n",
    "    unique_audio_paths = df_pd['speech_path'].unique()\n",
    "    \n",
    "    # Process each unique audio file\n",
    "    for audio_path in tqdm(unique_audio_paths, desc=\"Transcribing audio files\"):\n",
    "        # File is already a full path like '/merged_audio_files/merged_0000.wav'\n",
    "        file_name = os.path.basename(audio_path)\n",
    "        transcript_path = os.path.join(transcript_dir, file_name.replace('.wav', '.txt'))\n",
    "        \n",
    "        try:\n",
    "            # Remove leading slash if present for correct path joining\n",
    "            audio_path = audio_path.lstrip('/')\n",
    "            \n",
    "            # Transcribe using Whisper\n",
    "            transcription = whisper(audio_path)\n",
    "            transcript_text = transcription[\"text\"].strip()\n",
    "            \n",
    "            # Save transcription to file\n",
    "            with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(transcript_text)\n",
    "            \n",
    "            # Update all matching rows in the dataframe\n",
    "            mask = df_pd['speech_path'] == audio_path\n",
    "            df_pd.loc[mask, 'whisper_transcription'] = transcript_text\n",
    "            \n",
    "            print(f\"\\nProcessed {file_name}\")\n",
    "            print(f\"Transcription: {transcript_text[:100]}...\")  # Print first 100 chars\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError transcribing {audio_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert back to HuggingFace dataset\n",
    "    return Dataset.from_pandas(df_pd)\n",
    "\n",
    "def compare_transcriptions(df):\n",
    "    \"\"\"\n",
    "    Compare original text with Whisper transcriptions\n",
    "    \"\"\"\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'File': df['speech_path'],\n",
    "        'Original Text': df['text'],\n",
    "        'Whisper Transcription': df['whisper_transcription']\n",
    "    })\n",
    "    \n",
    "    comparison_df.to_csv('transcription_comparison.csv', index=False)\n",
    "    print(\"\\nSaved transcription comparison to transcription_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {use_gpu}\")\n",
    "\n",
    "try:\n",
    "    # Process the dataset\n",
    "        updated_dataset_trans = transcribe_audio_files(updated_dataset, use_gpu=use_gpu)\n",
    "    \n",
    "    # Create comparison file\n",
    "    # compare_transcriptions(updated_dataset_trans)\n",
    "    \n",
    "    # # Example of accessing the results\n",
    "    # print(\"\\nSample results:\")\n",
    "    # first_item = updated_dataset_trans[:1]\n",
    "    # print(f\"File: {first_item['speech_path'][0]}\")\n",
    "    # print(f\"Original text: {first_item['text'][0]}\")\n",
    "    # print(f\"Whisper transcription: {first_item['whisper_transcription'][0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing QA with LLama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_from_disk(\"/home/snp2453/slt/merged_df\")\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-1.5B-Instruct\",device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context0 = \"A lover to clock at struck. It was a fine clear night that were the only persons on the road, and they sonchered leisurely along, to avoid paying the price of fatigue for the recreation provided for the Toledans in the valley or on the banks of their river. Secure as he thought, in the careful administration of justice in that city, and the character of its well-disposed inhabitants, the good Hidalgo was far from thinking that any disaster could be followed as family. Rudolfo and his companions, with their faces muffled in their cloaks, stared rudely and insulently at the mother, the daughter and the servant made. In a moment, he communicated his thoughts to his companions, and in the next moment, they resolved to turn back and carry her off to please Rudolfo. For the rich who are open-handed, always find parasites ready to encourage their bad propensities. And thus to conceive this wicked design to communicate it, approve it, resolve on ravishing Leo Cadia, and to carry that design into effect was the work of a moment. They drew their swords, hid their faces in the flaps of their cloaks, turned back and soon came in front of the little party, who had not yet done giving thanks to God for their escape from those audacious men. Finally the one party went off exalting, and the other was left in desolation and woe. Rudolfo arrived at his own house without any impediment. And Leo Kadia's parents reached there, his heart broken and despairing. Meanwhile Rudolfo had Leo Kadia's safe in his custody. And in his own apartment, who touches me? Am I in bed? Mother, dear father, do you hear me?\"\n",
    "question = \"Where am I in this situation described in the text?\"\n",
    "gold_answer = \"You are in bed.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"You are an expert in precise question answering task. Given a context and a question, you need to give a direct answer, dont be verbose. Just give the answer directly. Context: {context0}. Question: {question}. Answer: \"},\n",
    "]\n",
    "pipe(messages,max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context0 = \"A lover to clock at struck. It was a fine clear night that were the only persons on the road, and they sonchered leisurely along, to avoid paying the price of fatigue for the recreation provided for the Toledans in the valley or on the banks of their river. Secure as he thought, in the careful administration of justice in that city, and the character of its well-disposed inhabitants, the good Hidalgo was far from thinking that any disaster could be followed as family. Rudolfo and his companions, with their faces muffled in their cloaks, stared rudely and insulently at the mother, the daughter and the servant made. In a moment, he communicated his thoughts to his companions, and in the next moment, they resolved to turn back and carry her off to please Rudolfo. For the rich who are open-handed, always find parasites ready to encourage their bad propensities. And thus to conceive this wicked design to communicate it, approve it, resolve on ravishing Leo Cadia, and to carry that design into effect was the work of a moment. They drew their swords, hid their faces in the flaps of their cloaks, turned back and soon came in front of the little party, who had not yet done giving thanks to God for their escape from those audacious men. Finally the one party went off exalting, and the other was left in desolation and woe. Rudolfo arrived at his own house without any impediment. And Leo Kadia's parents reached there, his heart broken and despairing. Meanwhile Rudolfo had Leo Kadia's safe in his custody. And in his own apartment, who touches me? Am I in bed? Mother, dear father, do you hear me?\"\n",
    "question = \"Who is in Rodolfo's custody and where are they being kept?\"\n",
    "gold_answer = \"Leocadia is safe in Rodolfo's custody, and they are being kept in his own apartment.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"You are an expert in precise question answering task. Given a context and a question, you need to give a direct answer, dont be verbose. Just give the answer directly. Context: {context0}. Question: {question}. Answer: \"},\n",
    "]\n",
    "ans = pipe(messages,max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    question = df[i]['question']\n",
    "    gt_answer = df[i]['answer']\n",
    "    text_file_path = df[i]['speech_path'].split(\"/\")[-1].split(\".\")[0]\n",
    "    full_path = \"/home/snp2453/slt/transcriptions/\" + text_file_path + \".txt\"\n",
    "    with open(full_path, 'r') as file:\n",
    "        content = file.readlines()\n",
    "    \n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"You are an expert in precise question answering task. Given a context and a question, you need to give a direct answer, dont be verbose. Just give the answer directly. Context: {content}. Question: {question}. Answer: \"},\n",
    "    ]\n",
    "    answer = pipe(messages,max_length=1024)\n",
    "    answer = answer[0]['generated_text'][1]['content']\n",
    "    answers.append(answer)    \n",
    "    # print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.add_column(\"LLama_Answers\",answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.push_to_hub(\"SP2001/SLT_merged_df\",private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval in Text Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = context0.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_embedding_store(model_name: str):\n",
    "    return HuggingFaceEmbedding(model_name=model_name, device = 'cuda:0',cache_folder=\"./\")\n",
    "\n",
    "def hf_embedding_ret(model_name: str):\n",
    "    return HuggingFaceEmbedding(model_name=model_name, device = 'cuda:1',cache_folder=\"./\")\n",
    "\n",
    "store_model = hf_embedding_store(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "ret_model = hf_embedding_ret(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "def retriever(documents, embedding_type=\"float\", model_name=\"embed-english-v3.0\"):\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        embed_model= store_model,\n",
    "    )\n",
    "    return VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=3,\n",
    "        embed_model = ret_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query = \"Where am I in this situation described in the text?\"\n",
    "    documents = [Document(text=context) for context in sentences]\n",
    "    retriever_int8 = retriever(documents, \"int8\")\n",
    "    retrieved_docs = retriever_int8.retrieve(query)\n",
    "    print(f\"retrieved_sentence is {[doc.text for doc in retrieved_docs]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query = \"Who is in Rodolfo's custody and where are they being kept?\"\n",
    "    documents = [Document(text=context) for context in sentences]\n",
    "    retriever_int8 = retriever(documents, \"int8\")\n",
    "    retrieved_docs = retriever_int8.retrieve(query)\n",
    "    print(f\"retrieved_sentence is {[doc.text for doc in retrieved_docs]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Speech Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/wav2vec2-large-xlsr-53-german\"\n",
    "feature_extractor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "i= feature_extractor(train_dataset[:10][\"speech\"], return_tensors=\"pt\", padding=True, \n",
    "                                 feature_size=1, sampling_rate=16000 )\n",
    "model(**i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medhallu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
